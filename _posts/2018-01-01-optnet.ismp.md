---
layout: post
title: OptNet, end-to-end task-based learning, and control (ISMP)
date: 2018-01-01
powerpoint: 2018.optnet.ismp.pptx
pdf: 2018.optnet.ismp.pdf
abstract: >-
    Deep learning and end-to-end architectures provide a general and powerful way
    of implementing most modern machine learning tasks with a relatively small set
    of differentiable operations. These operations are usually simple affine
    operations composed with pointwise nonlinearities like the ReLU or sigmoid
    function. While general and successful, the drawbacks of these operations are
    plentiful, as the resulting learned modules can be uninterpretable and
    difficult to inject domainspecific knowledge into. This talk presents OptNet, a
    new paradigm for deep learning that integrates the solution of optimization
    problems "into the loop." OptNet allows domain knowledge in the form of
    learnable constrained optimization problems to be integrated into larger
    end-to-end architectures.  We will first discuss the new OptNet primitive
    operations in the form of learning the parameters of a constrained convex
    quadratic program from data. Then we will show applications of applying these
    primitive operations in non-convex stochastic optimization and control.
---
